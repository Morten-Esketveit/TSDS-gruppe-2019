{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Downloading and cleaning data on flights and airports*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import urllib.request as req"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data on flights and airports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flights, 2007\n",
    "\n",
    "Data comes from the US Bureau of Transportation Statistics (BTS) but is gathered with complete data up to 2007 at\n",
    "\n",
    "http://stat-computing.org/dataexpo/2009/the-data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Flights\n",
    "filename = '2007.csv.bz2'\n",
    "url = 'http://stat-computing.org/dataexpo/2009/'+str(filename)\n",
    "req.urlretrieve(url,filename)\n",
    "Flights = pd.read_csv(filename)\n",
    "\n",
    "# Pickle un-cleaned: too big for Github, as >100 MB\n",
    "Flights.to_pickle(\"Flights.pkl\")\n",
    "\n",
    "print('Rows:', len(Flights.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flights.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Unique journeys for price scraping***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose columns for later\n",
    "scrape_data = Flights[[\"Year\",\"Month\",\"DayofMonth\",\"DayOfWeek\",\"DepTime\",\"UniqueCarrier\",\"Origin\",\"Dest\",\"Distance\"]]\n",
    "\n",
    "# Prepare data for scraping\n",
    "scrape_data[\"Journey\"] = scrape_data[\"Origin\"]+Flights[\"Dest\"]\n",
    "scrape_data = scrape_data.drop_duplicates(subset = \"Journey\", keep = \"first\").reset_index(drop=True)\n",
    "\n",
    "# Pickle it for price scraping\n",
    "scrape_data.to_pickle(\"scrape_data.pkl\")\n",
    "\n",
    "scrape_data.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flights, 2018\n",
    "\n",
    "The newest data from BTS must be downloaded manually from https://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236\n",
    "\n",
    "First tick the following \"Field Names\":\n",
    "\n",
    "* \"Year\"\n",
    "* \"Month\"\n",
    "* \"DayofMonth\"\n",
    "* \"DayOfWeek\"\n",
    "* \"Reporting_Airline\" (UniqueCarrier)\n",
    "* \"Origin\"\n",
    "* \"Dest\"\n",
    "* \"DepTime\"\n",
    "* \"Distance\"\n",
    "\n",
    "For each month of 2018:\n",
    "\n",
    "1. Press download\n",
    "2. Locate (or create) the folder \"Data\" within the cd of this .ipynb-file\n",
    "3. Name the download after the number of the month, i.e. from 1-12\n",
    "4. Save it in the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for m in range(1,13):\n",
    "    df = pd.read_csv(\"Data/\"+str(m)+\".zip\")\n",
    "    data.append(df)\n",
    "Flights18 = pd.concat(data, axis=0).drop('Unnamed: 9', axis=1).reset_index(drop=True)\n",
    "Flights18.columns = [\"Year\",\"Month\",\"DayofMonth\",\"DayOfWeek\",\"UniqueCarrier\",\"Origin\",\"Dest\",\"DepTime\",\"Distance\"]\n",
    "Flights18 = Flights18[[\"Year\",\"Month\",\"DayofMonth\",\"DayOfWeek\",\"DepTime\",\"UniqueCarrier\",\"Origin\",\"Dest\",\"Distance\"]]\n",
    "\n",
    "Flights18.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airports\n",
    "\n",
    "Data comes from https://openflights.org/data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "airports_raw = pd.read_csv('https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat', header = None) #load data\n",
    "\n",
    "# Assign column names\n",
    "airports_raw.columns = [\"Airport_id\",\"Name\",\"City\",\"Country\",\"3DigitId\",\"4DigitId\",\"Lat\",\"Lon\",\"Altitude_ft\",\"Timezone\",\"DST\",\"TZ\",\"Type\",\"Source\"] \n",
    "\n",
    "# Pick relevant columns for analysis\n",
    "airports_df = airports_raw[[\"Airport_id\",\"Name\",\"City\",\"Country\",\"3DigitId\",\"Lat\",\"Lon\",\"Altitude_ft\"]]\n",
    "\n",
    "# Limit to US Airports\n",
    "airports_US = airports_df[airports_df['Country']=='United States'].reset_index(drop=True)\n",
    "\n",
    "# Pickle the dataframe for future use\n",
    "airports_US.to_pickle(\"Airports.pkl\")\n",
    "\n",
    "print('Rows, all:', len(airports_df.index))\n",
    "print('Rows, US: ', len(airports_US.index))\n",
    "print('- of which missing 3DigitId:', airports_US['3DigitId'].value_counts(dropna=False).nlargest(1)[0])\n",
    "print('- of which containing 3DigitId:', airports_US['3DigitId'].nunique())\n",
    "airports_US.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = pd.read_pickle(\"Airports.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data on US airports from stat-computing.org\n",
    "# filename = 'airports.csv'\n",
    "# url = 'http://stat-computing.org/dataexpo/2009/'+str(filename)\n",
    "# req.urlretrieve(url,filename)\n",
    "# Airports = pd.read_csv(filename)\n",
    "\n",
    "# print('Shape:', Airports.shape)\n",
    "# Airports.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning for flights data, 2007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count number of flights between airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select variables from which to create network. Must contain two columns of nodes, since \n",
    "# nx.from_pandas_edgelist() views each row as an edge. Can also contains attributes, e.g. Distance. \n",
    "FlightsNx = Flights[[\"Origin\",\"Dest\",\"Distance\"]]\n",
    "\n",
    "# Take counts (i.e. how many origin to destination flights in the dataset)\n",
    "counts = pd.DataFrame(Flights.groupby([\"Origin\",\"Dest\"])[\"Year\"].count())\n",
    "counts.columns = [\"count\"]\n",
    "FlightsNx = FlightsNx.merge(counts, how = 'left', on = [\"Origin\",\"Dest\"])\n",
    "\n",
    "# Drop duplicates \n",
    "FlightsNx = FlightsNx.drop_duplicates()\n",
    "\n",
    "# Count number of take-offs from origin: \n",
    "orig_takeoff = pd.DataFrame(FlightsNx.groupby([\"Origin\"])[\"count\"].sum())\n",
    "orig_takeoff.columns = [\"origin_takeoff\"]\n",
    "# Merge to network dataset: \n",
    "FlightsNx = FlightsNx.merge(orig_takeoff, how = \"left\", on = \"Origin\")\n",
    "\n",
    "# Count number of take-offs from dest\n",
    "dest_takeoff = pd.DataFrame(FlightsNx.groupby([\"Origin\"])[\"count\"].sum())\n",
    "dest_takeoff.columns = [\"dest_takeoff\"]\n",
    "FlightsNx = FlightsNx.merge(dest_takeoff, how = \"left\", left_on = \"Dest\", right_on = \"Origin\")\n",
    "\n",
    "# Count number of landings at destination\n",
    "dest_lands = pd.DataFrame(FlightsNx.groupby([\"Dest\"])[\"count\"].sum())\n",
    "dest_lands.columns = [\"dest_landing\"]\n",
    "FlightsNx = FlightsNx.merge(dest_lands, how = \"left\", on = \"Dest\")\n",
    "\n",
    "orig_lands = pd.DataFrame(FlightsNx.groupby([\"Dest\"])[\"count\"].sum())\n",
    "orig_lands.columns = [\"orig_landing\"]\n",
    "FlightsNx = FlightsNx.merge(orig_lands, how = \"left\", left_on = \"Origin\", right_on = \"Dest\")\n",
    "\n",
    "# Construct totals (flights)\n",
    "FlightsNx[\"Origin_flights\"] = FlightsNx[\"origin_takeoff\"]+FlightsNx[\"orig_landing\"]\n",
    "FlightsNx[\"Destination_flights\"] = FlightsNx[\"dest_takeoff\"]+FlightsNx[\"dest_landing\"]\n",
    "\n",
    "# Drop auxiliary vars\n",
    "FlightsNx.drop([\"origin_takeoff\",\"dest_takeoff\",\"dest_landing\",\"orig_landing\"], axis = 1, inplace = True)\n",
    "\n",
    "print('Rows:', len(FlightsNx.index))\n",
    "FlightsNx.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement as network\n",
    "For function documentation, see:  \n",
    "https://networkx.github.io/documentation/stable/reference/generated/networkx.convert_matrix.from_pandas_edgelist.html?highlight=from_pandas_edgelist#networkx.convert_matrix.from_pandas_edgelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Network = nx.from_pandas_edgelist(FlightsNx, source = \"Origin\", target = \"Dest\", edge_attr = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs: https://networkx.github.io/documentation/stable/reference/algorithms/generated/networkx.algorithms.centrality.degree_centrality.html#networkx.algorithms.centrality.degree_centrality \n",
    "# Find degree: \n",
    "degree_dict = nx.algorithms.centrality.degree_centrality(Network) \n",
    "degree_values = list(degree_dict.values())\n",
    "# Create dataframe\n",
    "degrees = pd.DataFrame(degree_values)\n",
    "degrees.columns = [\"degree\"]\n",
    "degrees[\"degree_unnormalized\"] = degrees[\"degree\"]*(len(Network.nodes)-1)\n",
    "degrees[\"airport\"] = degree_dict.keys()\n",
    "\n",
    "# Pickle for descriptive\n",
    "degrees.to_pickle(\"Degrees.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add degree to FlightsNx data\n",
    "FlightsNx = FlightsNx.merge(degrees, how = \"left\", left_on = \"Origin\", right_on = \"airport\")\n",
    "FlightsNx = FlightsNx.drop(\"airport\", axis = 1)\n",
    "FlightsNx = FlightsNx.rename(columns = {\"degree\": \"origin_degree\", \"degree_unnormalized\": \"origin_degree_unn\"})\n",
    "\n",
    "FlightsNx = FlightsNx.merge(degrees, how = \"left\", left_on = \"Dest\", right_on = \"airport\")\n",
    "FlightsNx = FlightsNx.drop(\"airport\", axis = 1).reset_index(drop=True)\n",
    "FlightsNx = FlightsNx.rename(columns = {\"degree\": \"dest_degree\", \"degree_unnormalized\": \"dest_degree_unn\"})\n",
    "\n",
    "FlightsNx.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned to pickle\n",
    "FlightsNx.to_pickle(\"FlightsNx.pkl\") # only 0.4 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning for flights data, 2018 (same code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Count number of flights between airports\n",
    "\n",
    "# Select variables from which to create network. Must contain two columns of nodes, since \n",
    "# nx.from_pandas_edgelist() views each row as an edge. Can also contains attributes, e.g. Distance. \n",
    "FlightsNx = Flights18[[\"Origin\",\"Dest\",\"Distance\"]]\n",
    "\n",
    "# Take counts (i.e. how many origin to destination flights in the dataset)\n",
    "counts = pd.DataFrame(Flights.groupby([\"Origin\",\"Dest\"])[\"Year\"].count())\n",
    "counts.columns = [\"count\"]\n",
    "FlightsNx = FlightsNx.merge(counts, how = 'left', on = [\"Origin\",\"Dest\"])\n",
    "\n",
    "# Drop duplicates \n",
    "FlightsNx = FlightsNx.drop_duplicates()\n",
    "\n",
    "# Count number of take-offs from origin: \n",
    "orig_takeoff = pd.DataFrame(FlightsNx.groupby([\"Origin\"])[\"count\"].sum())\n",
    "orig_takeoff.columns = [\"origin_takeoff\"]\n",
    "# Merge to network dataset: \n",
    "FlightsNx = FlightsNx.merge(orig_takeoff, how = \"left\", on = \"Origin\")\n",
    "\n",
    "# Count number of take-offs from dest\n",
    "dest_takeoff = pd.DataFrame(FlightsNx.groupby([\"Origin\"])[\"count\"].sum())\n",
    "dest_takeoff.columns = [\"dest_takeoff\"]\n",
    "FlightsNx = FlightsNx.merge(dest_takeoff, how = \"left\", left_on = \"Dest\", right_on = \"Origin\")\n",
    "\n",
    "# Count number of landings at destination\n",
    "dest_lands = pd.DataFrame(FlightsNx.groupby([\"Dest\"])[\"count\"].sum())\n",
    "dest_lands.columns = [\"dest_landing\"]\n",
    "FlightsNx = FlightsNx.merge(dest_lands, how = \"left\", on = \"Dest\")\n",
    "\n",
    "orig_lands = pd.DataFrame(FlightsNx.groupby([\"Dest\"])[\"count\"].sum())\n",
    "orig_lands.columns = [\"orig_landing\"]\n",
    "FlightsNx = FlightsNx.merge(orig_lands, how = \"left\", left_on = \"Origin\", right_on = \"Dest\")\n",
    "\n",
    "# Construct totals (flights)\n",
    "FlightsNx[\"Origin_flights\"] = FlightsNx[\"origin_takeoff\"]+FlightsNx[\"orig_landing\"]\n",
    "FlightsNx[\"Destination_flights\"] = FlightsNx[\"dest_takeoff\"]+FlightsNx[\"dest_landing\"]\n",
    "\n",
    "# Drop auxiliary vars\n",
    "FlightsNx.drop([\"origin_takeoff\",\"dest_takeoff\",\"dest_landing\",\"orig_landing\"], axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "### Implement as network\n",
    "\n",
    "Network = nx.from_pandas_edgelist(FlightsNx, source = \"Origin\", target = \"Dest\", edge_attr = True)\n",
    "\n",
    "\n",
    "### Degree distribution\n",
    "\n",
    "# docs: https://networkx.github.io/documentation/stable/reference/algorithms/generated/networkx.algorithms.centrality.degree_centrality.html#networkx.algorithms.centrality.degree_centrality \n",
    "# Find degree: \n",
    "degree_dict = nx.algorithms.centrality.degree_centrality(Network) \n",
    "degree_values = list(degree_dict.values())\n",
    "# Create dataframe\n",
    "degrees = pd.DataFrame(degree_values)\n",
    "degrees.columns = [\"degree\"]\n",
    "degrees[\"degree_unnormalized\"] = degrees[\"degree\"]*(len(Network.nodes)-1)\n",
    "degrees[\"airport\"] = degree_dict.keys()\n",
    "\n",
    "# Pickle for descriptive\n",
    "degrees.to_pickle(\"Degrees18.pkl\")\n",
    "\n",
    "\n",
    "# Add degree to FlightsNx data\n",
    "FlightsNx = FlightsNx.merge(degrees, how = \"left\", left_on = \"Origin\", right_on = \"airport\")\n",
    "FlightsNx = FlightsNx.drop(\"airport\", axis = 1)\n",
    "FlightsNx = FlightsNx.rename(columns = {\"degree\": \"origin_degree\", \"degree_unnormalized\": \"origin_degree_unn\"})\n",
    "\n",
    "FlightsNx = FlightsNx.merge(degrees, how = \"left\", left_on = \"Dest\", right_on = \"airport\")\n",
    "FlightsNx = FlightsNx.drop(\"airport\", axis = 1).reset_index(drop=True)\n",
    "FlightsNx = FlightsNx.rename(columns = {\"degree\": \"dest_degree\", \"degree_unnormalized\": \"dest_degree_unn\"})\n",
    "\n",
    "\n",
    "# Save cleaned to pickle\n",
    "FlightsNx.to_pickle(\"FlightsNx18.pkl\")\n",
    "\n",
    "FlightsNx.tail(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
